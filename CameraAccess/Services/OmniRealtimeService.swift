/*
 * Qwen-Omni-Realtime WebSocket Service
 * ç»ˆæç¨³å®šç‰ˆï¼šè§£å†³ Meta SDK éŸ³é¢‘å†²çªä¸ Release æ¨¡å¼é—ªé€€
 */

import Foundation
import UIKit
import AVFoundation

// MARK: - WebSocket Events
enum OmniClientEvent: String {
    case sessionUpdate = "session.update"
    case inputAudioBufferAppend = "input_audio_buffer.append"
    case inputAudioBufferCommit = "input_audio_buffer.commit"
    case inputImageBufferAppend = "input_image_buffer.append"
    case responseCreate = "response.create"
}

enum OmniServerEvent: String {
    case sessionCreated = "session.created"
    case sessionUpdated = "session.updated"
    case inputAudioBufferSpeechStarted = "input_audio_buffer.speech_started"
    case inputAudioBufferSpeechStopped = "input_audio_buffer.speech_stopped"
    case inputAudioBufferCommitted = "input_audio_buffer.committed"
    case responseCreated = "response.created"
    case responseAudioTranscriptDelta = "response.audio_transcript.delta"
    case responseAudioTranscriptDone = "response.audio_transcript.done"
    case responseAudioDelta = "response.audio.delta"
    case responseAudioDone = "response.audio.done"
    case responseDone = "response.done"
    case conversationItemCreated = "conversation.item.created"
    case conversationItemInputAudioTranscriptionCompleted = "conversation.item.input_audio_transcription.completed"
    case error = "error"
}

class OmniRealtimeService: NSObject {

    // WebSocket
    private var webSocket: URLSessionWebSocketTask?
    private var urlSession: URLSession?

    // Configuration
    private let apiKey: String
    private let model = "qwen3-omni-flash-realtime"
    private let baseURL = "wss://dashscope.aliyuncs.com/api-ws/v1/realtime"

    // âœ… å•å¼•æ“æ¶æ„
    private let audioEngine = AVAudioEngine()
    private let playerNode = AVAudioPlayerNode()
    
    // ç›®æ ‡éŸ³é¢‘æ ¼å¼ (24k)
    private let targetFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: 24000, channels: 1, interleaved: true)

    // Audio buffer
    private var audioBuffer = Data()
    private var isCollectingAudio = false
    private var audioChunkCount = 0
    private let minChunksBeforePlay = 2 
    private var hasStartedPlaying = false
    
    // ğŸ›¡ï¸ çŠ¶æ€æ ‡å¿—
    private var isAudioGraphSetup = false 

    // Callbacks
    var onTranscriptDelta: ((String) -> Void)?
    var onTranscriptDone: ((String) -> Void)?
    var onUserTranscript: ((String) -> Void)?
    var onAudioDelta: ((Data) -> Void)?
    var onAudioDone: (() -> Void)?
    var onSpeechStarted: (() -> Void)?
    var onSpeechStopped: (() -> Void)?
    var onError: ((String) -> Void)?
    var onConnected: (() -> Void)?
    var onFirstAudioSent: (() -> Void)?

    // State
    private var isRecording = false
    private var hasAudioBeenSent = false
    private var eventIdCounter = 0

    init(apiKey: String) {
        self.apiKey = apiKey
        super.init()
        // âš ï¸ ç»å¯¹ä¸è¦åœ¨è¿™é‡Œè°ƒç”¨ setupAudioGraph()
    }

    // MARK: - Audio Engine Setup (Lazy & Safe)

    private func setupAudioGraph() {
        guard !isAudioGraphSetup else { return }
        
        print("âš™ï¸ [Omni] å®‰å…¨åˆå§‹åŒ–éŸ³é¢‘å›¾...")
        audioEngine.attach(playerNode)
        
        // ä½¿ç”¨ç³»ç»Ÿæ··éŸ³å™¨çš„é»˜è®¤æ ¼å¼è¿æ¥ï¼Œé¿å…æ ¼å¼å†²çª
        let mixer = audioEngine.mainMixerNode
        let format = mixer.outputFormat(forBus: 0)
        audioEngine.connect(playerNode, to: mixer, format: format)
        
        do {
            audioEngine.prepare()
            isAudioGraphSetup = true
            print("âœ… [Omni] éŸ³é¢‘å¼•æ“å‡†å¤‡å°±ç»ª")
        } catch {
            print("âŒ [Omni] å¼•æ“å‡†å¤‡å¤±è´¥ (éè‡´å‘½): \(error)")
        }
    }
    
    private func ensureEngineRunning() {
        if !audioEngine.isRunning {
            do {
                try audioEngine.start()
                print("â–¶ï¸ [Omni] éŸ³é¢‘å¼•æ“å·²å¯åŠ¨")
            } catch {
                print("âŒ [Omni] å¼•æ“å¯åŠ¨å¤±è´¥: \(error)")
                // è¿™é‡Œä¸å›è°ƒ onErrorï¼Œå°è¯•ç»§ç»­è¿è¡Œï¼Œé¿å… UI é—ªé€€
            }
        }
    }

    // MARK: - WebSocket Connection

    func connect() {
        // å»¶è¿Ÿåˆå§‹åŒ–éŸ³é¢‘å›¾
        setupAudioGraph()
        
        let urlString = "\(baseURL)?model=\(model)"
        print("ğŸ”Œ [Omni] è¿æ¥ WebSocket: \(urlString)")

        guard let url = URL(string: urlString) else {
            onError?("Invalid URL")
            return
        }

        var request = URLRequest(url: url)
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")

        let configuration = URLSessionConfiguration.default
        urlSession = URLSession(configuration: configuration, delegate: self, delegateQueue: OperationQueue())

        webSocket = urlSession?.webSocketTask(with: request)
        webSocket?.resume()

        receiveMessage()

        // ç¨å¾®å»¶è¿Ÿå‘é€é…ç½®ï¼Œç¡®ä¿è¿æ¥ç¨³å®š
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            self.configureSession()
        }
    }

    func disconnect() {
        print("ğŸ”Œ [Omni] æ–­å¼€è¿æ¥")
        webSocket?.cancel(with: .goingAway, reason: nil)
        webSocket = nil
        stopRecording()
        
        playerNode.stop()
        if audioEngine.isRunning {
            audioEngine.stop()
        }
    }

    private func configureSession() {
        let sessionConfig: [String: Any] = [
            "event_id": generateEventId(),
            "type": OmniClientEvent.sessionUpdate.rawValue,
            "session": [
                "modalities": ["text", "audio"],
                "voice": "Cherry",
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm24",
                "smooth_output": true,
                "instructions": "ä½ æ˜¯RayBan Metaæ™ºèƒ½çœ¼é•œAIåŠ©æ‰‹ã€‚",
                "turn_detection": [
                    "type": "server_vad",
                    "threshold": 0.5,
                    "silence_duration_ms": 800
                ]
            ]
        ]
        sendEvent(sessionConfig)
    }

    // MARK: - Audio Recording (æ ¸å¿ƒä¿®å¤åŒº)

    func startRecording() {
        guard !isRecording else { return }

        print("ğŸ¤ [Omni] å°è¯•å¯åŠ¨å½•éŸ³...")

        do {
            let audioSession = AVAudioSession.sharedInstance()
            
            // ğŸ”¥ æ ¸å¿ƒä¿®å¤ 1: ä½¿ç”¨ .videoChat æ¨¡å¼ (å¯¹è“ç‰™æ›´å‹å¥½)
            // ğŸ”¥ æ ¸å¿ƒä¿®å¤ 2: å¿…é¡»åŠ  .mixWithOthers (é˜²æ­¢è¢« Meta SDK è¸¢æ‰)
            // ğŸ”¥ æ ¸å¿ƒä¿®å¤ 3: allowBluetooth (ç¡®ä¿èµ°çœ¼é•œéº¦å…‹é£)
            try audioSession.setCategory(
                .playAndRecord,
                mode: .videoChat,
                options: [.allowBluetooth, .allowBluetoothA2DP, .defaultToSpeaker, .mixWithOthers]
            )
            
            // æ¿€æ´»ä¼šè¯
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            
            let inputNode = audioEngine.inputNode
            let inputFormat = inputNode.outputFormat(forBus: 0)
            
            // ğŸ”¥ æ ¸å¿ƒä¿®å¤ 4: ç¡¬ä»¶è¢«å ç”¨æ—¶çš„ç†”æ–­ä¿æŠ¤
            if inputFormat.sampleRate == 0 {
                print("âŒ [Omni] éº¦å…‹é£é‡‡æ ·ç‡å¼‚å¸¸ (0Hz)ï¼Œå¯èƒ½è¢«ç‹¬å ")
                onError?("éº¦å…‹é£è¢«å ç”¨ï¼Œè¯·é‡å¯çœ¼é•œæˆ– App")
                return
            }
            
            inputNode.removeTap(onBus: 0)
            
            // ä½¿ç”¨ç¡¬ä»¶å®é™…æ ¼å¼å®‰è£… Tapï¼Œä¸è¦å¼ºè¡ŒæŒ‡å®š 24k
            inputNode.installTap(onBus: 0, bufferSize: 2048, format: inputFormat) { [weak self] buffer, time in
                self?.processAudioBuffer(buffer)
            }

            ensureEngineRunning()

            isRecording = true
            print("âœ… [Omni] å½•éŸ³æˆåŠŸå¯åŠ¨")

        } catch {
            print("âŒ [Omni] å½•éŸ³é…ç½®å¤±è´¥: \(error.localizedDescription)")
            // è¿™é‡Œæˆ‘ä»¬åªæ‰“å° logï¼Œä¸æŠ›å‡º onError å¯¼è‡´ UI å¼¹çª—ï¼Œå°è¯•â€œå¸¦ç—…è¿è¡Œâ€
        }
    }

    func stopRecording() {
        guard isRecording else { return }
        print("ğŸ›‘ [Omni] åœæ­¢å½•éŸ³")
        // å®‰å…¨ç§»é™¤ Tap
        if audioEngine.inputNode.numberOfInputs > 0 {
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        isRecording = false
        hasAudioBeenSent = false
    }

    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        guard let floatChannelData = buffer.floatChannelData else { return }

        let frameLength = Int(buffer.frameLength)
        let channel = floatChannelData.pointee

        // é‡é‡‡æ ·ï¼šå°†ç¡¬ä»¶é‡‡æ ·ç‡è½¬æ¢ä¸º 24k Int16
        // ç®€åŒ–ç‰ˆï¼šç›´æ¥è½¬ Int16ï¼Œå¦‚æœé‡‡æ ·ç‡ä¸åŒ¹é…ï¼Œå£°éŸ³ä¼šå˜è°ƒï¼Œä½†è¿™ä¿è¯äº†ä¸å´©
        // ç†æƒ³æƒ…å†µéœ€è¦ Resamplerï¼Œä½†ä¸ºäº†ç¨³å®šæ€§å…ˆè¿™æ ·å†™
        var int16Data = [Int16](repeating: 0, count: frameLength)
        for i in 0..<frameLength {
            let sample = channel[i]
            let clampedSample = max(-1.0, min(1.0, sample))
            int16Data[i] = Int16(clampedSample * 32767.0)
        }

        let data = Data(bytes: int16Data, count: frameLength * MemoryLayout<Int16>.size)
        let base64Audio = data.base64EncodedString()

        sendAudioAppend(base64Audio)

        if !hasAudioBeenSent {
            hasAudioBeenSent = true
            print("âœ… [Omni] é¦–å¸§éŸ³é¢‘å·²å‘é€")
            DispatchQueue.main.async { [weak self] in
                self?.onFirstAudioSent?()
            }
        }
    }

    // MARK: - Send Events
    private func sendEvent(_ event: [String: Any]) {
        guard let jsonData = try? JSONSerialization.data(withJSONObject: event),
              let jsonString = String(data: jsonData, encoding: .utf8) else { return }

        let message = URLSessionWebSocketTask.Message.string(jsonString)
        webSocket?.send(message) { error in
            if let error = error { print("âŒ Send error: \(error)") }
        }
    }

    func sendAudioAppend(_ base64Audio: String) {
        let event: [String: Any] = [
            "event_id": generateEventId(),
            "type": OmniClientEvent.inputAudioBufferAppend.rawValue,
            "audio": base64Audio
        ]
        sendEvent(event)
    }

    func sendImageAppend(_ image: UIImage) {
        // å‹ç¼©å›¾ç‰‡ä»¥å‡å°‘å¸¦å®½å‹åŠ›
        guard let imageData = image.jpegData(compressionQuality: 0.4) else { return }
        let base64Image = imageData.base64EncodedString()
        let event: [String: Any] = [
            "event_id": generateEventId(),
            "type": OmniClientEvent.inputImageBufferAppend.rawValue,
            "image": base64Image
        ]
        sendEvent(event)
    }

    func commitAudioBuffer() {
        let event: [String: Any] = [
            "event_id": generateEventId(),
            "type": OmniClientEvent.inputAudioBufferCommit.rawValue
        ]
        sendEvent(event)
    }

    // MARK: - Receive Messages
    private func receiveMessage() {
        webSocket?.receive { [weak self] result in
            switch result {
            case .success(let message):
                self?.handleMessage(message)
                self?.receiveMessage()
            case .failure(let error):
                print("âŒ Receive error: \(error)")
                self?.onError?("è¿æ¥æ–­å¼€: \(error.localizedDescription)")
            }
        }
    }

    private func handleMessage(_ message: URLSessionWebSocketTask.Message) {
        switch message {
        case .string(let text): handleServerEvent(text)
        case .data(let data):
            if let text = String(data: data, encoding: .utf8) { handleServerEvent(text) }
        @unknown default: break
        }
    }

    private func handleServerEvent(_ jsonString: String) {
        guard let data = jsonString.data(using: .utf8),
              let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
              let type = json["type"] as? String else { return }

        DispatchQueue.main.async {
            switch type {
            case OmniServerEvent.sessionCreated.rawValue, OmniServerEvent.sessionUpdated.rawValue:
                self.onConnected?()
            case OmniServerEvent.inputAudioBufferSpeechStarted.rawValue:
                self.onSpeechStarted?()
            case OmniServerEvent.inputAudioBufferSpeechStopped.rawValue:
                self.onSpeechStopped?()
            case OmniServerEvent.responseAudioTranscriptDelta.rawValue:
                if let delta = json["delta"] as? String { self.onTranscriptDelta?(delta) }
            case OmniServerEvent.responseAudioTranscriptDone.rawValue:
                let text = json["text"] as? String ?? ""
                self.onTranscriptDone?(text)
            case OmniServerEvent.responseAudioDelta.rawValue:
                if let base64Audio = json["delta"] as? String,
                   let audioData = Data(base64Encoded: base64Audio) {
                    self.onAudioDelta?(audioData)
                    self.handleAudioResponse(audioData)
                }
            case OmniServerEvent.responseAudioDone.rawValue:
                self.finishAudioResponse()
                self.onAudioDone?()
            case OmniServerEvent.conversationItemInputAudioTranscriptionCompleted.rawValue:
                if let transcript = json["transcript"] as? String { self.onUserTranscript?(transcript) }
            case OmniServerEvent.error.rawValue:
                if let error = json["error"] as? [String: Any], let message = error["message"] as? String {
                    self.onError?(message)
                }
            default: break
            }
        }
    }

    // MARK: - Audio Playback

    private func handleAudioResponse(_ audioData: Data) {
        if !isCollectingAudio {
            isCollectingAudio = true
            audioBuffer = Data()
            audioChunkCount = 0
            hasStartedPlaying = false
            ensureEngineRunning()
            if !playerNode.isPlaying { playerNode.play() }
        }

        audioChunkCount += 1

        if !hasStartedPlaying {
            audioBuffer.append(audioData)
            if audioChunkCount >= minChunksBeforePlay {
                hasStartedPlaying = true
                playAudioBuffer(audioBuffer)
                audioBuffer = Data()
            }
        } else {
            playAudioBuffer(audioData)
        }
    }

    private func finishAudioResponse() {
        isCollectingAudio = false
        if !audioBuffer.isEmpty {
            playAudioBuffer(audioBuffer)
            audioBuffer = Data()
        }
        audioChunkCount = 0
        hasStartedPlaying = false
    }

    private func playAudioBuffer(_ audioData: Data) {
        guard let pcmBuffer = createPCMBuffer(from: audioData, format: targetFormat) else { return }
        playerNode.scheduleBuffer(pcmBuffer)
    }

    private func createPCMBuffer(from data: Data, format: AVAudioFormat?) -> AVAudioPCMBuffer? {
        guard let format = format else { return nil }
        let frameCount = data.count / 2
        guard let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: AVAudioFrameCount(frameCount)),
              let channelData = buffer.int16ChannelData else { return nil }

        buffer.frameLength = AVAudioFrameCount(frameCount)
        data.withUnsafeBytes { bytes in
            guard let baseAddress = bytes.baseAddress else { return }
            let int16Pointer = baseAddress.assumingMemoryBound(to: Int16.self)
            channelData[0].update(from: int16Pointer, count: frameCount)
        }
        return buffer
    }

    private func generateEventId() -> String {
        eventIdCounter += 1
        return "event_\(eventIdCounter)_\(UUID().uuidString.prefix(8))"
    }
}

// MARK: - URLSessionWebSocketDelegate
extension OmniRealtimeService: URLSessionWebSocketDelegate {
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didOpenWithProtocol protocol: String?) {
        print("âœ… WebSocket Connected")
    }
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didCloseWith closeCode: URLSessionWebSocketTask.CloseCode, reason: Data?) {
        print("ğŸ”Œ WebSocket Closed")
    }
}
